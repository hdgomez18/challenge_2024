{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO0dfwduO/4EXycQy3sEB4L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hdgomez18/challenge_2024/blob/main/etl_withPyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **instalacion libreria**"
      ],
      "metadata": {
        "id": "Q6fKa8ev8K_z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Bjfk6M359z9",
        "outputId": "3d33b257-cfc6-4244-c943-fb5178615ac4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.3.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.3-py2.py3-none-any.whl size=317840625 sha256=52d7b90d9af64101dd254bab21bfcb61d9ab5439173ce5bce61690690c733d63\n",
            "  Stored in directory: /root/.cache/pip/wheels/1b/3a/92/28b93e2fbfdbb07509ca4d6f50c5e407f48dce4ddbda69a4ab\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.3\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Crear session de spark"
      ],
      "metadata": {
        "id": "6dHY-YZV8ZzV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession, Window\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql import types as T, functions as F\n",
        "\n",
        "\n",
        "spSession = SparkSession \\\n",
        "            .builder \\\n",
        "            .appName(\"pyspark_code\") \\\n",
        "            .getOrCreate()"
      ],
      "metadata": {
        "id": "BTtaKZqS6HVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create dataframe\n",
        "data = [(\"Hernan\",35),(\"Nicolas\",43),('Dario',22)]\n",
        "columns = ['Name','Age']\n",
        "df_ex = spSession.createDataFrame(data,columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yz2ppLhWg_UI",
        "outputId": "e175ee7b-2d5b-434f-8009-2bcd6b09f530"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+\n",
            "|   Name|Age|\n",
            "+-------+---+\n",
            "| Hernan| 35|\n",
            "|Nicolas| 43|\n",
            "|  Dario| 22|\n",
            "+-------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_ex.where(col('Age') > 30).show()\n",
        "\n",
        "df_ex.select(max('Age').alias('age')).show()\n",
        "\n",
        "df_ex = df_ex.withColumn('Salary',\n",
        "                 when(col('Name').isin('Hernan'),300000)\n",
        "                 .when(col('Name').isin('Nicolas'),200000)\n",
        "                 .when(col('Name').isin('Dario'),500000))\n",
        "\n",
        "df_ex = df_ex.withColumn('Deparment',\n",
        "                 when(col('Name').isin('Hernan'),'Sales')\n",
        "                 .when(col('Name').isin('Nicolas'),'Administrative')\n",
        "                 .when(col('Name').isin('Dario'),'Production'))\n",
        "\n",
        "df_ex.groupBy('Deparment').agg(sum('Salary').alias('Salary')).show()\n",
        "\n",
        "df_ex = df_ex.withColumn(\"row_numberstr\", F.row_number().over(Window.partitionBy('Name',).orderBy('Name')))\n",
        "\n",
        "w = Window.orderBy(col(\"row_numberstr\"))\n",
        "\n",
        "df_ex = df_ex.withColumn(\"id\", F.row_number().over(w))\n",
        "\n",
        "df_ex = df_ex.drop('row_numberstr')\n",
        "\n",
        "df_ex = df_ex.select('id','Name','Age','Deparment','Salary')\n",
        "\n",
        "df_ex = df_ex.withColumn('Deparment',regexp_replace('Deparment','Administrative','Admin'))\n",
        "\n",
        "df_ex = df_ex.withColumnRenamed('Deparment','department')\n",
        "\n",
        "df_ex1 = df_ex1.withColumn('id',lit(1))\n",
        "\n",
        "df_ex1 = df_ex1.withColumn('date_hiring',lit('2023-10-01'))\n",
        "\n",
        "df_ex1 = df_ex1.withColumn('date_hiring',col('date_hiring').cast('date'))\n",
        "\n",
        "df_ex3 = df_ex.alias('df_a').join(df_ex1.alias('df_b'),['id'],'left').select('df_a.*','df_b.date_hiring')\n",
        "\n",
        "df_ex3.groupBy('department').pivot('id').agg(sum('salary')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a2NIJvgiGHS",
        "outputId": "47527df1-8e36-49d2-9394-939f182b105b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+------+------+\n",
            "|department|     1|     2|     3|\n",
            "+----------+------+------+------+\n",
            "|Production|500000|  NULL|  NULL|\n",
            "|     Sales|  NULL|300000|  NULL|\n",
            "|     Admin|  NULL|  NULL|200000|\n",
            "+----------+------+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def return_middle(word):\n",
        "    middle= int(len(word)/2)\n",
        "    if middle % 2 == 0:\n",
        "        return word[middle-1:middle+1]\n",
        "    else: word[middle]\n",
        "\n",
        "return_middle('hernan')"
      ],
      "metadata": {
        "id": "yR_j4QIL4qEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def word_bk(word):\n",
        "    wordbk = word[: : -1]\n",
        "    print(wordbk)\n",
        "\n",
        "word_bk('algo')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXG6w_MLQJ3U",
        "outputId": "4f4e1429-de43-444e-c172-aa8f6e6789d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ogla\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        " **Carga de datos**"
      ],
      "metadata": {
        "id": "_SUV5jRd8eVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_depa = spSession.read.csv(\"departments.csv\", header = 'false',sep= ';')\n",
        "df_hire = spSession.read.csv(\"hired_employees.csv\", header = 'false',sep= ';')\n",
        "df_jobs = spSession.read.csv(\"jobs.csv\", header = 'false',sep= ';')"
      ],
      "metadata": {
        "id": "abbBDWNU8kQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from os import truncate\n",
        "from pyspark.sql.types import StringType,BooleanType,DateType,IntegerType,DoubleType,LongType\n",
        "from pyspark.sql.functions import *\n",
        "#delete nulls\n",
        "df_depa = df_depa.where(col('_c0').isNotNull())\n",
        "#replace columns names\n",
        "df_depa = df_depa.withColumnRenamed('_c0','id')\n",
        "df_depa = df_depa.withColumnRenamed('_c1','department')\n",
        "#cast columns to integer\n",
        "df_depa = df_depa.withColumn('id',col('id').cast(IntegerType()))\n",
        "df_depa.show(truncate= False)\n",
        "#replace columns names\n",
        "df_hire = df_hire.withColumnRenamed('_c0','id')\n",
        "df_hire = df_hire.withColumnRenamed('_c1','name')\n",
        "df_hire = df_hire.withColumnRenamed('_c2','datetime')\n",
        "df_hire = df_hire.withColumnRenamed('_c3','department_id')\n",
        "df_hire = df_hire.withColumnRenamed('_c4','job_id')\n",
        "#separate the rows with nulls\n",
        "df_without_name = df_hire.where(col('name').isNull() | col('datetime').isNull() |  col('department_id').isNull() |  col('job_id').isNull())\n",
        "\n",
        "df_hire_f = df_hire.where(col('name').isNotNull() | col('datetime').isNotNull() |  col('department_id').isNotNull() |  col('job_id').isNotNull())\n",
        "#cast columns to integer\n",
        "df_hire_f = df_hire_f.withColumn('id',col('id').cast(IntegerType()))\n",
        "df_hire_f = df_hire_f.withColumn('department_id',col('department_id').cast(IntegerType()))\n",
        "df_hire_f = df_hire_f.withColumn('job_id',col('job_id').cast(IntegerType()))\n",
        "\n",
        "df_jobs.show(truncate= False)\n",
        "df_f2.count()\n",
        "\n",
        "#replace columns names\n",
        "df_jobs = df_jobs.withColumnRenamed('_c0','id')\n",
        "df_jobs = df_jobs.withColumnRenamed('_c1','job')\n",
        "#cast columns to integer\n",
        "df_jobs = df_jobs.withColumn('id',col('id').cast(IntegerType()))"
      ],
      "metadata": {
        "id": "GTvWYdCs-13Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CREATE DB AND TABLES"
      ],
      "metadata": {
        "id": "Kto04DYya0LH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "db_name = \"COMPANY_AAA\"\n",
        "spSession.sql(f\"CREATE DATABASE IF NOT EXISTS {db_name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6hbLnfrNecw",
        "outputId": "e41acde6-cff0-4907-aaa9-e8f60c57c6f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from os.path import abspath\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# warehouse_location points to the default location for managed databases and tables\n",
        "warehouse_location = abspath('spark-warehouse')\n",
        "\n",
        "# Create spark session with hive enabled\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"SparkByExamples.com\") \\\n",
        "    .config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
        "    .enableHiveSupport() \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "68hy-O83XWOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CREATE TABLE JOBS\n",
        "df_jobs.write.mode('overwrite') \\\n",
        "    .saveAsTable(\"company_aaa.jobs\")\n",
        "#CREATE TABLE DEPARMENTS\n",
        "df_depa.write.mode('overwrite') \\\n",
        "    .saveAsTable(\"company_aaa.deparments\")\n",
        "#CREATE TABLE hired_employees\n",
        "df_hire_f.write.mode('overwrite') \\\n",
        "    .saveAsTable(\"company_aaa.hired_employees\")\n",
        "#CREATE TABLE hired_employees_withNulls\n",
        "df_without_name.write.mode('overwrite') \\\n",
        "    .saveAsTable(\"company_aaa.employeesNulls\")\n"
      ],
      "metadata": {
        "id": "MTelZ5IuZ_b0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "save backup to parquet"
      ],
      "metadata": {
        "id": "SMsa09t4dHTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_jobs.write.format(\"parquet\").save(\"/content/backup/jobs.parquet\")\n",
        "df_depa.write.format(\"parquet\").save(\"/content/backup/deparment.parquet\")\n",
        "df_hire_f.write.format(\"parquet\").save(\"/content/backup/employees.parquet\")\n",
        "df_without_name.write.format(\"parquet\").save(\"/content/backup/employees_null.parquet\")"
      ],
      "metadata": {
        "id": "330dIhHAdF8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "create function to restore table from backup"
      ],
      "metadata": {
        "id": "VnI6jE0fiXwA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def restore_table (name_table: str):\n",
        "    df_backup = spark.read.parquet('/content/backup/'+name_table+'.parquet')\n",
        "    return print('Please use the next dataframe: df_backup to consult the data of '+name_table)\n",
        "\n",
        "restore_table('deparment')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12IApMsUiXAM",
        "outputId": "2c2aa099-b58e-4042-c5d9-6420d9a014fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please use the next dataframe: df_backup to consult the data of deparment\n"
          ]
        }
      ]
    }
  ]
}