{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKdivPtJRTgZROIvmz8iXQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hdgomez18/challenge_2024/blob/main/etl_withPyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **instalacion libreria**"
      ],
      "metadata": {
        "id": "Q6fKa8ev8K_z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Bjfk6M359z9",
        "outputId": "2c06d945-e46e-430d-ffdf-df7555b704cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=948a93ab4af811933f1276e02ce5b10d7b045b989a9cba7d5695218db3fea0fd\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.2\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Crear session de spark"
      ],
      "metadata": {
        "id": "6dHY-YZV8ZzV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext\n",
        "\n",
        "\n",
        "spSession = SparkSession \\\n",
        "            .builder \\\n",
        "            .appName(\"pyspark_code\") \\\n",
        "            .getOrCreate()"
      ],
      "metadata": {
        "id": "BTtaKZqS6HVd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Carga de datos**"
      ],
      "metadata": {
        "id": "_SUV5jRd8eVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_depa = spSession.read.csv(\"departments.csv\", header = 'false',sep= ';')\n",
        "df_hire = spSession.read.csv(\"hired_employees.csv\", header = 'false',sep= ';')\n",
        "df_jobs = spSession.read.csv(\"jobs.csv\", header = 'false',sep= ';')"
      ],
      "metadata": {
        "id": "abbBDWNU8kQi"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from os import truncate\n",
        "from pyspark.sql.types import StringType,BooleanType,DateType,IntegerType,DoubleType,LongType\n",
        "from pyspark.sql.functions import *\n",
        "#delete nulls\n",
        "df_depa = df_depa.where(col('_c0').isNotNull())\n",
        "#replace columns names\n",
        "df_depa = df_depa.withColumnRenamed('_c0','id')\n",
        "df_depa = df_depa.withColumnRenamed('_c1','department')\n",
        "#cast columns to integer\n",
        "df_depa = df_depa.withColumn('id',col('id').cast(IntegerType()))\n",
        "df_depa.show(truncate= False)\n",
        "#replace columns names\n",
        "df_hire = df_hire.withColumnRenamed('_c0','id')\n",
        "df_hire = df_hire.withColumnRenamed('_c1','name')\n",
        "df_hire = df_hire.withColumnRenamed('_c2','datetime')\n",
        "df_hire = df_hire.withColumnRenamed('_c3','department_id')\n",
        "df_hire = df_hire.withColumnRenamed('_c4','job_id')\n",
        "#separate the rows with nulls\n",
        "df_without_name = df_hire.where(col('name').isNull() | col('datetime').isNull() |  col('department_id').isNull() |  col('job_id').isNull())\n",
        "\n",
        "df_hire_f = df_hire.where(col('name').isNotNull() | col('datetime').isNotNull() |  col('department_id').isNotNull() |  col('job_id').isNotNull())\n",
        "#cast columns to integer\n",
        "df_hire_f = df_hire_f.withColumn('id',col('id').cast(IntegerType()))\n",
        "df_hire_f = df_hire_f.withColumn('department_id',col('department_id').cast(IntegerType()))\n",
        "df_hire_f = df_hire_f.withColumn('job_id',col('job_id').cast(IntegerType()))\n",
        "\n",
        "df_jobs.show(truncate= False)\n",
        "df_f2.count()\n",
        "\n",
        "#replace columns names\n",
        "df_jobs = df_jobs.withColumnRenamed('_c0','id')\n",
        "df_jobs = df_jobs.withColumnRenamed('_c1','job')\n",
        "#cast columns to integer\n",
        "df_jobs = df_jobs.withColumn('id',col('id').cast(IntegerType()))"
      ],
      "metadata": {
        "id": "GTvWYdCs-13Y"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CREATE DB AND TABLES"
      ],
      "metadata": {
        "id": "Kto04DYya0LH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "db_name = \"COMPANY_AAA\"\n",
        "spSession.sql(f\"CREATE DATABASE IF NOT EXISTS {db_name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6hbLnfrNecw",
        "outputId": "e41acde6-cff0-4907-aaa9-e8f60c57c6f4"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from os.path import abspath\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# warehouse_location points to the default location for managed databases and tables\n",
        "warehouse_location = abspath('spark-warehouse')\n",
        "\n",
        "# Create spark session with hive enabled\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"SparkByExamples.com\") \\\n",
        "    .config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
        "    .enableHiveSupport() \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "68hy-O83XWOc"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CREATE TABLE JOBS\n",
        "df_jobs.write.mode('overwrite') \\\n",
        "    .saveAsTable(\"company_aaa.jobs\")\n",
        "#CREATE TABLE DEPARMENTS\n",
        "df_depa.write.mode('overwrite') \\\n",
        "    .saveAsTable(\"company_aaa.deparments\")\n",
        "#CREATE TABLE hired_employees\n",
        "df_hire_f.write.mode('overwrite') \\\n",
        "    .saveAsTable(\"company_aaa.hired_employees\")\n",
        "#CREATE TABLE hired_employees_withNulls\n",
        "df_without_name.write.mode('overwrite') \\\n",
        "    .saveAsTable(\"company_aaa.employeesNulls\")\n"
      ],
      "metadata": {
        "id": "MTelZ5IuZ_b0"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "save backup to parquet"
      ],
      "metadata": {
        "id": "SMsa09t4dHTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_jobs.write.format(\"parquet\").save(\"/content/backup/jobs.parquet\")\n",
        "df_depa.write.format(\"parquet\").save(\"/content/backup/deparment.parquet\")\n",
        "df_hire_f.write.format(\"parquet\").save(\"/content/backup/employees.parquet\")\n",
        "df_without_name.write.format(\"parquet\").save(\"/content/backup/employees_null.parquet\")"
      ],
      "metadata": {
        "id": "330dIhHAdF8Y"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "create function to restore table from backup"
      ],
      "metadata": {
        "id": "VnI6jE0fiXwA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def restore_table (name_table: str):\n",
        "    df_backup = spark.read.parquet('/content/backup/'+name_table+'.parquet')\n",
        "    return print('Please use the next dataframe: df_backup to consult the data of '+name_table)\n",
        "\n",
        "restore_table('deparment')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12IApMsUiXAM",
        "outputId": "2c2aa099-b58e-4042-c5d9-6420d9a014fb"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please use the next dataframe: df_backup to consult the data of deparment\n"
          ]
        }
      ]
    }
  ]
}